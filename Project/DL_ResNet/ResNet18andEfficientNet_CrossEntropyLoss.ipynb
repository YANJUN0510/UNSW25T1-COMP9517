{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Authorize Drive and set up pre-environments**"
      ],
      "metadata": {
        "id": "s1gTNtCpobV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount and authorize Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2hx_fjc89st",
        "outputId": "99ae99d0-d33c-4ec6-8176-039120662f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the necessary packages are installed\n",
        "!pip install --quiet --upgrade torch torchvision matplotlib seaborn scikit-learn tqdm pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjNuSM6w0ly2",
        "outputId": "ae176d6b-1623-4ada-8805-19842a952be1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rOatkzBV0hqc"
      },
      "outputs": [],
      "source": [
        "# Loading Kit\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms as tfs\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torchvision import models\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import recall_score, f1_score,accuracy_score,precision_score,confusion_matrix,classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iYeSKzUq0hqd"
      },
      "outputs": [],
      "source": [
        "# Setting up training equipment\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Construction data collection**"
      ],
      "metadata": {
        "id": "F1X3ttg9oQoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Aerial_Landscapes.zip\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the compressed file\n",
        "zip_path = '/content/drive/MyDrive/Aerial_Landscapes.zip'\n",
        "extract_path = '/content/aerial_data'\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Unzipping complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1QBhLIv9HG2",
        "outputId": "66a1aa67-9cb1-4be4-b86c-efa6a7d80952"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train / val / test folders\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume that the folder after decompression is extract_path, which contains the folders of each type of image\n",
        "source_dir = os.path.join(extract_path, 'Aerial_Landscapes')\n",
        "split_root = '/content/split_data'\n",
        "\n",
        "for class_name in os.listdir(source_dir):\n",
        "    class_path = os.path.join(source_dir, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = [img for img in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, img))]\n",
        "\n",
        "    train_imgs, temp_imgs = train_test_split(images, test_size=0.3, random_state=42)\n",
        "    val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
        "\n",
        "    for split, split_imgs in zip(['train', 'val', 'test'], [train_imgs, val_imgs, test_imgs]):\n",
        "        split_dir = os.path.join(split_root, split, class_name)\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "        for img in split_imgs:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(split_dir, img))\n",
        "\n",
        "print(\"Dataset splitting completed！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPXEND6N9OgM",
        "outputId": "5e1b17c0-59bc-4437-8351-7224e77de645"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splitting completed！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "skASWBGd0hqe",
        "outputId": "a2f2268a-f808-4c9c-bcc7-e06c8761f898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create df successfully！\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        filename        class  label\n",
              "0  /content/split_data/train/Agriculture/180.jpg  Agriculture      0\n",
              "1  /content/split_data/train/Agriculture/636.jpg  Agriculture      0\n",
              "2  /content/split_data/train/Agriculture/472.jpg  Agriculture      0\n",
              "3  /content/split_data/train/Agriculture/678.jpg  Agriculture      0\n",
              "4  /content/split_data/train/Agriculture/773.jpg  Agriculture      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6fb295c-ae1b-4749-bcd3-628224adb4bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/split_data/train/Agriculture/180.jpg</td>\n",
              "      <td>Agriculture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/split_data/train/Agriculture/636.jpg</td>\n",
              "      <td>Agriculture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/split_data/train/Agriculture/472.jpg</td>\n",
              "      <td>Agriculture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/split_data/train/Agriculture/678.jpg</td>\n",
              "      <td>Agriculture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/split_data/train/Agriculture/773.jpg</td>\n",
              "      <td>Agriculture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6fb295c-ae1b-4749-bcd3-628224adb4bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c6fb295c-ae1b-4749-bcd3-628224adb4bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c6fb295c-ae1b-4749-bcd3-628224adb4bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f1f8e69-18fd-46ff-90b3-b6ba4eeab8bb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f1f8e69-18fd-46ff-90b3-b6ba4eeab8bb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f1f8e69-18fd-46ff-90b3-b6ba4eeab8bb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 8400,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8400,\n        \"samples\": [\n          \"/content/split_data/train/Agriculture/791.jpg\",\n          \"/content/split_data/train/Mountain/161.jpg\",\n          \"/content/split_data/train/Lake/490.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Mountain\",\n          \"Port\",\n          \"Agriculture\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Read the data set and create a data table\n",
        "def create_df(root):\n",
        "    label = 0\n",
        "    label_list = []\n",
        "    filepath_list = []\n",
        "    class_list = []\n",
        "\n",
        "    for class_name in sorted(os.listdir(root)):\n",
        "        class_path = os.path.join(root, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, filename)\n",
        "            filepath_list.append(image_path)\n",
        "            class_list.append(class_name)\n",
        "            label_list.append(label)\n",
        "        label += 1\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"filename\": filepath_list,\n",
        "        \"class\": class_list,\n",
        "        \"label\": label_list\n",
        "    })\n",
        "\n",
        "df_train = create_df('/content/split_data/train')\n",
        "df_val = create_df('/content/split_data/val')\n",
        "df_test = create_df('/content/split_data/test')\n",
        "\n",
        "print(\"Create df successfully！\")\n",
        "df_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Class Weights\n",
        "\n",
        "# Get the labels of all training sets\n",
        "all_labels = df_train['label'].to_numpy()\n",
        "\n",
        "# Get the labels of all training sets\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "\n",
        "# Convert to PyTorch Tensor and throw it to GPU\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ],
      "metadata": {
        "id": "gzRuf7Y1kGYc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9pUEtknt0hqe"
      },
      "outputs": [],
      "source": [
        "# Define image preprocessing\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize = transforms.Normalize(mean, std)\n",
        "\n",
        "resize_H, resized_W = 224,224\n",
        "resize = transforms.Resize([resize_H, resized_W])\n",
        "\n",
        "transformations_train = transforms.Compose([resize,\n",
        "                                    tfs.RandomHorizontalFlip(0.2),\n",
        "                                    tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.RandomAffine(0, None, (0.7, 1.1), (0, 30),fill=(127,127,127)),\n",
        "                                    normalize])\n",
        "transformations_test = transforms.Compose([resize,\n",
        "                                    transforms.ToTensor(),\n",
        "                                    normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BipydD4L0hqf"
      },
      "outputs": [],
      "source": [
        "# Convert image to Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['filename']\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "\n",
        "        # Open the image and convert it to RGB\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset\n",
        "train_dataset = ImageDataset(df_train, transformations_train)\n",
        "val_dataset = ImageDataset(df_val, transformations_test)\n",
        "test_dataset = ImageDataset(df_test, transformations_test)\n",
        "\n",
        "# Create DataLoader (automatic batch loading)\n",
        "batch_size = 64\n",
        "num_workers = 2  # You can set it smaller according to RAM\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
      ],
      "metadata": {
        "id": "npdVd00GIFfE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gMH3Kmn0hqh"
      },
      "source": [
        "# **RESNET**-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTZOvpT60hqi",
        "outputId": "82645df7-f266-457a-88a6-f65b5dd54af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Model 1: ResNet18\n",
        "net = models.resnet18(pretrained=True)\n",
        "net.fc =  nn.Linear(net.fc.in_features,15)\n",
        "\n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "my_lr= 1e-4\n",
        "bs= batch_size\n",
        "epochs = 10\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "945f_wKj0hqj",
        "outputId": "495ead2e-ca41-4d29-8f76-2462a82a6763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 2.3740, Train Accuracy: 0.2201, Valid Loss: 1.7097, Valid Accuracy: 0.3911, Recall: 0.3911, F1-score: 0.3849\n",
            "Epoch [2/10], Train Loss: 1.3781, Train Accuracy: 0.5348, Valid Loss: 1.1571, Valid Accuracy: 0.6256, Recall: 0.6256, F1-score: 0.6153\n",
            "Epoch [3/10], Train Loss: 0.9307, Train Accuracy: 0.6974, Valid Loss: 1.4892, Valid Accuracy: 0.5639, Recall: 0.5639, F1-score: 0.5375\n",
            "Epoch [4/10], Train Loss: 0.7079, Train Accuracy: 0.7700, Valid Loss: 1.7127, Valid Accuracy: 0.4756, Recall: 0.4756, F1-score: 0.4832\n",
            "Epoch [5/10], Train Loss: 0.5689, Train Accuracy: 0.8133, Valid Loss: 1.4822, Valid Accuracy: 0.5683, Recall: 0.5683, F1-score: 0.5279\n",
            "Epoch [6/10], Train Loss: 0.4802, Train Accuracy: 0.8473, Valid Loss: 0.9196, Valid Accuracy: 0.7111, Recall: 0.7111, F1-score: 0.6989\n",
            "Epoch [7/10], Train Loss: 0.3963, Train Accuracy: 0.8724, Valid Loss: 2.2969, Valid Accuracy: 0.4011, Recall: 0.4011, F1-score: 0.3750\n",
            "Epoch [8/10], Train Loss: 0.3578, Train Accuracy: 0.8813, Valid Loss: 2.3174, Valid Accuracy: 0.5350, Recall: 0.5350, F1-score: 0.5266\n",
            "Epoch [9/10], Train Loss: 0.3031, Train Accuracy: 0.9005, Valid Loss: 0.5956, Valid Accuracy: 0.8294, Recall: 0.8294, F1-score: 0.8335\n",
            "Epoch [10/10], Train Loss: 0.2805, Train Accuracy: 0.9073, Valid Loss: 4.9621, Valid Accuracy: 0.2972, Recall: 0.2972, F1-score: 0.2653\n"
          ]
        }
      ],
      "source": [
        "# Model training loop (ResNet18)\n",
        "def train_model():\n",
        "    train_loss, valid_loss = [], []\n",
        "    train_acc, valid_acc = [], []\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == target).sum().item()\n",
        "            total_train += target.size(0)\n",
        "\n",
        "        train_loss_epoch = running_train_loss / len(train_loader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_loss.append(train_loss_epoch)\n",
        "        train_acc.append(train_accuracy)\n",
        "\n",
        "        net.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        running_valid_loss = 0.0\n",
        "        correct_valid = 0\n",
        "        total_valid = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(val_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = net(data)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "                running_valid_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_valid += (predicted == target).sum().item()\n",
        "                total_valid += target.size(0)\n",
        "\n",
        "        recall = recall_score(all_labels, all_preds, average='macro')\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        valid_loss_epoch = running_valid_loss / len(val_loader)\n",
        "        valid_accuracy = correct_valid / total_valid\n",
        "        valid_loss.append(valid_loss_epoch)\n",
        "        valid_acc.append(valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_epoch:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Valid Loss: {valid_loss_epoch:.4f}, Valid Accuracy: {valid_accuracy:.4f}, \"\n",
        "              f\"Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh9KS4wU0hqj",
        "outputId": "3fec625f-3878-4d1d-b9db-cde08ae584e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 4.9838, Test Accuracy: 0.3000\n",
            "Test Recall: 0.3000\n",
            "Test F1-score: 0.2647\n",
            "Confusion Matrix:\n",
            "[[ 37   0  50   0   0   0   0   0   0   0   0   0  33   0   0]\n",
            " [  0   3  30   0   0   0   0   0   0   0   0   0  87   0   0]\n",
            " [  0   0 114   0   0   0   0   0   0   0   0   0   6   0   0]\n",
            " [  0   0   9   5   0   0   0   0   0   7   0   1  98   0   0]\n",
            " [  0   0  51   0  69   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0  89   0   7   2   1   0   0   2   0   0  19   0   0]\n",
            " [  0   0 115   0   1   0   0   0   0   0   0   0   4   0   0]\n",
            " [  0   0  29   0   0   0   0  15   0   0   0   0  76   0   0]\n",
            " [  2   0  65   0   0   0   0   0   2   0   0   1  49   0   1]\n",
            " [  0   0  79   0   4   0   0   0   0  12   0   0  25   0   0]\n",
            " [  0   0   1   0   0   0   0  10   0   0  93   0  16   0   0]\n",
            " [  0   0  12   0   0   0   0   2   0   0   5  62  39   0   0]\n",
            " [  0   0   6   0   0   0   0   0   0   0   0   0 114   0   0]\n",
            " [  0   2  11   1   0   0   0   1   0   1   0   0 103   1   0]\n",
            " [  0   0  78   0   0   0   0   0   0   0   0   0  31   0  11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9487    0.3083    0.4654       120\n",
            "           1     0.6000    0.0250    0.0480       120\n",
            "           2     0.1543    0.9500    0.2654       120\n",
            "           3     0.8333    0.0417    0.0794       120\n",
            "           4     0.8519    0.5750    0.6866       120\n",
            "           5     1.0000    0.0167    0.0328       120\n",
            "           6     0.0000    0.0000    0.0000       120\n",
            "           7     0.5357    0.1250    0.2027       120\n",
            "           8     1.0000    0.0167    0.0328       120\n",
            "           9     0.5455    0.1000    0.1690       120\n",
            "          10     0.9490    0.7750    0.8532       120\n",
            "          11     0.9688    0.5167    0.6739       120\n",
            "          12     0.1629    0.9500    0.2780       120\n",
            "          13     1.0000    0.0083    0.0165       120\n",
            "          14     0.9167    0.0917    0.1667       120\n",
            "\n",
            "    accuracy                         0.3000      1800\n",
            "   macro avg     0.6978    0.3000    0.2647      1800\n",
            "weighted avg     0.6978    0.3000    0.2647      1800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test model (ResNet18)\n",
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Initialize variables for test evaluation\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "running_test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "# Loop through the test data and calculate predictions\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "        running_test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_test += (predicted == target).sum().item()\n",
        "        total_test += target.size(0)\n",
        "\n",
        "# Calculate metrics\n",
        "test_loss = running_test_loss / len(test_loader)\n",
        "test_accuracy = correct_test / total_test\n",
        "test_recall = recall_score(all_labels, all_preds, average='macro')\n",
        "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-score: {test_f1:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Classification Report:\\n{classification_report(all_labels, all_preds, digits=4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the ResNet's results into variables and finally draw a comparison picture\n",
        "resnet_test_results = {\n",
        "    'name': 'ResNet18',\n",
        "    'accuracy': test_accuracy,\n",
        "    'recall': test_recall,\n",
        "    'f1': test_f1,\n",
        "    'loss': test_loss\n",
        "}"
      ],
      "metadata": {
        "id": "QXUUikH0fqRP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6eHN1LZ0hqj"
      },
      "source": [
        "# **Efficientnet_b0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFNotzQU0hqk",
        "outputId": "1c659779-4c21-416e-e21c-9ecbd4dfe43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 143MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 2.2994, Train Accuracy: 0.2663, Valid Loss: 1.5018, Valid Accuracy: 0.5189, Recall: 0.5189, F1-score: 0.5012\n",
            "Epoch [2/10], Train Loss: 1.3339, Train Accuracy: 0.5698, Valid Loss: 0.8052, Valid Accuracy: 0.7550, Recall: 0.7550, F1-score: 0.7523\n",
            "Epoch [3/10], Train Loss: 0.9509, Train Accuracy: 0.6914, Valid Loss: 0.5612, Valid Accuracy: 0.8278, Recall: 0.8278, F1-score: 0.8287\n",
            "Epoch [4/10], Train Loss: 0.7158, Train Accuracy: 0.7737, Valid Loss: 0.4996, Valid Accuracy: 0.8356, Recall: 0.8356, F1-score: 0.8370\n",
            "Epoch [5/10], Train Loss: 0.5934, Train Accuracy: 0.8075, Valid Loss: 0.3607, Valid Accuracy: 0.8911, Recall: 0.8911, F1-score: 0.8906\n",
            "Epoch [6/10], Train Loss: 0.5000, Train Accuracy: 0.8381, Valid Loss: 0.6413, Valid Accuracy: 0.7817, Recall: 0.7817, F1-score: 0.7828\n",
            "Epoch [7/10], Train Loss: 0.4353, Train Accuracy: 0.8575, Valid Loss: 0.2604, Valid Accuracy: 0.9244, Recall: 0.9244, F1-score: 0.9241\n",
            "Epoch [8/10], Train Loss: 0.3805, Train Accuracy: 0.8744, Valid Loss: 0.9885, Valid Accuracy: 0.6983, Recall: 0.6983, F1-score: 0.6950\n",
            "Epoch [9/10], Train Loss: 0.3425, Train Accuracy: 0.8891, Valid Loss: 0.3858, Valid Accuracy: 0.8739, Recall: 0.8739, F1-score: 0.8690\n",
            "Epoch [10/10], Train Loss: 0.3109, Train Accuracy: 0.9006, Valid Loss: 0.2464, Valid Accuracy: 0.9272, Recall: 0.9272, F1-score: 0.9269\n"
          ]
        }
      ],
      "source": [
        "# Model 2: EfficientNet-B0\n",
        "net = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "net.classifier[1] = nn.Linear(net.classifier[1].in_features, 15)\n",
        "\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "my_lr = 1e-4\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "train_loss, valid_loss = [], []\n",
        "train_acc, valid_acc = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    net.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == target).sum().item()\n",
        "        total_train += target.size(0)\n",
        "\n",
        "    train_loss_epoch = running_train_loss / len(train_loader)\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_loss.append(train_loss_epoch)\n",
        "    train_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "    net.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    running_valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = net(data)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "            running_valid_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_valid += (predicted == target).sum().item()\n",
        "            total_valid += target.size(0)\n",
        "\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    valid_loss_epoch = running_valid_loss / len(val_loader)\n",
        "    valid_accuracy = correct_valid / total_valid\n",
        "    valid_loss.append(valid_loss_epoch)\n",
        "    valid_acc.append(valid_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_epoch:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Valid Loss: {valid_loss_epoch:.4f}, Valid Accuracy: {valid_accuracy:.4f}, \"\n",
        "          f\"Recall: {recall:.4f}, F1-score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIChdhio0hqk",
        "outputId": "3a75d431-708b-4402-d6e7-f95cf0788a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2283, Test Accuracy: 0.9322\n",
            "Test Recall: 0.9322\n",
            "Test F1-score: 0.9321\n",
            "Confusion Matrix:\n",
            "[[113   2   0   0   0   0   3   0   0   1   0   0   0   0   1]\n",
            " [  1 108   0   3   0   0   0   0   0   0   0   0   7   0   1]\n",
            " [  0   0 118   1   0   0   0   0   0   0   0   0   0   0   1]\n",
            " [  0   0   0 115   0   0   0   0   0   0   0   1   4   0   0]\n",
            " [  1   0   2   2  96   0   1   0   2  16   0   0   0   0   0]\n",
            " [  0   1   0   0   0 114   2   0   1   0   0   1   0   1   0]\n",
            " [  5   0   0   0   1   2 108   0   1   0   0   0   0   0   3]\n",
            " [  0   2   0   0   0   0   0 113   0   0   0   0   3   1   1]\n",
            " [  3   0   1   0   0   0   1   0 111   0   0   0   0   0   4]\n",
            " [  1   1   0   0   1   1   0   1   0 109   0   0   0   0   6]\n",
            " [  0   0   1   1   0   0   0   0   0   0 117   1   0   0   0]\n",
            " [  1   1   2   0   0   0   0   0   1   0   0 115   0   0   0]\n",
            " [  0   4   0   1   0   0   0   1   0   0   0   0 114   0   0]\n",
            " [  0   0   0   2   0   0   0   0   0   0   0   0   0 118   0]\n",
            " [  3   0   1   0   0   1   0   3   3   0   0   0   0   0 109]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8828    0.9417    0.9113       120\n",
            "           1     0.9076    0.9000    0.9038       120\n",
            "           2     0.9440    0.9833    0.9633       120\n",
            "           3     0.9200    0.9583    0.9388       120\n",
            "           4     0.9796    0.8000    0.8807       120\n",
            "           5     0.9661    0.9500    0.9580       120\n",
            "           6     0.9391    0.9000    0.9191       120\n",
            "           7     0.9576    0.9417    0.9496       120\n",
            "           8     0.9328    0.9250    0.9289       120\n",
            "           9     0.8651    0.9083    0.8862       120\n",
            "          10     1.0000    0.9750    0.9873       120\n",
            "          11     0.9746    0.9583    0.9664       120\n",
            "          12     0.8906    0.9500    0.9194       120\n",
            "          13     0.9833    0.9833    0.9833       120\n",
            "          14     0.8651    0.9083    0.8862       120\n",
            "\n",
            "    accuracy                         0.9322      1800\n",
            "   macro avg     0.9339    0.9322    0.9321      1800\n",
            "weighted avg     0.9339    0.9322    0.9321      1800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test model (EfficientNet-B0)\n",
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Initialize variables for test evaluation\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "running_test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "# Loop through the test data and calculate predictions\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "        running_test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_test += (predicted == target).sum().item()\n",
        "        total_test += target.size(0)\n",
        "\n",
        "# Calculate metrics\n",
        "test_loss = running_test_loss / len(test_loader)\n",
        "test_accuracy = correct_test / total_test\n",
        "test_recall = recall_score(all_labels, all_preds, average='macro')\n",
        "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-score: {test_f1:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Classification Report:\\n{classification_report(all_labels, all_preds, digits=4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Efficientnet's results into variables and finally draw a comparison picture\n",
        "efficientnet_test_results = {\n",
        "    'name': 'EfficientNetB0',\n",
        "    'accuracy': test_accuracy,\n",
        "    'recall': test_recall,\n",
        "    'f1': test_f1,\n",
        "    'loss': test_loss\n",
        "}"
      ],
      "metadata": {
        "id": "zazukOjAfw_G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparative picture**"
      ],
      "metadata": {
        "id": "4guN5a53oDLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "PS4Tgxe00hqk",
        "outputId": "b66dce39-d188-467d-fb97-229f86ee1e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting to compare the effectiveness of two models\n",
        "def plot_test_metrics(resnet_results, efficientnet_results):\n",
        "    metrics = ['accuracy', 'recall', 'f1', 'loss']\n",
        "    resnet_values = [resnet_results[m] for m in metrics]\n",
        "    effnet_values = [efficientnet_results[m] for m in metrics]\n",
        "\n",
        "    x = range(len(metrics))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    bar_width = 0.35\n",
        "\n",
        "    plt.bar([i - bar_width/2 for i in x], resnet_values, width=bar_width, label='ResNet18')\n",
        "    plt.bar([i + bar_width/2 for i in x], effnet_values, width=bar_width, label='EfficientNetB0')\n",
        "\n",
        "    plt.xticks(ticks=x, labels=[m.capitalize() for m in metrics])\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"📊 Model Test Performance Comparison\")\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "plot_test_metrics(resnet_test_results, efficientnet_test_results)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}