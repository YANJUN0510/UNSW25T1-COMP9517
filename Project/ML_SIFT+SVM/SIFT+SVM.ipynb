{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddc7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import(\n",
    "accuracy_score,\n",
    "precision_score,\n",
    "recall_score,\n",
    "f1_score,confusion_matrix,\n",
    "classification_report)\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf57fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "N_CLUSTERS=80 #视觉词袋大小\n",
    "MAX_SAMPLES=5000 #用于训练词袋的最大样本数\n",
    "TEST_SIZE =0.2 # 测试集比例\n",
    "RANDOM_STATE =42 #随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7373cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(dataset_path, test_size=0.2, sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    按类别划分训练集和测试集，确保每个类别的样本按比例分割\n",
    "    数据抽样功能：sample_ratio: 每类样本的抽样比例 (0.0-1.0)\n",
    "    数据集结构应为:\n",
    "    dataset/\n",
    "        class1/\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            ...\n",
    "    返回:\n",
    "        (train_images, train_labels), (test_images, test_labels), classes\n",
    "    \"\"\"\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    classes = sorted(os.listdir(dataset_path))\n",
    "\n",
    "    for class_id, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        img_paths = glob(os.path.join(class_dir, '*.jpg'))\n",
    "\n",
    "        # 按类别分层划分\n",
    "        if len(img_paths) == 0:\n",
    "            continue  # 跳过空目录\n",
    "\n",
    "        # 随机打乱路径\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        np.random.shuffle(img_paths)\n",
    "\n",
    "        # 先进行数据抽样\n",
    "        n_samples = int(len(img_paths) * sample_ratio)\n",
    "        sampled_paths = img_paths[:max(n_samples, 1)]  # 确保至少取1个样本\n",
    "\n",
    "        # 再划分训练测试集\n",
    "        split_idx = int(len(sampled_paths) * (1 - test_size))  # 计算划分点\n",
    "        train_paths = sampled_paths[:split_idx]\n",
    "        test_paths = sampled_paths[split_idx:] if split_idx < len(sampled_paths) else []\n",
    "\n",
    "        # 加载训练集图像\n",
    "        for path in train_paths:\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None:\n",
    "                train_images.append(img)\n",
    "                train_labels.append(class_id)\n",
    "            else:\n",
    "                print('图片读取失败！')\n",
    "\n",
    "        # 加载测试集图像\n",
    "        for path in test_paths:\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None:\n",
    "                test_images.append(img)\n",
    "                test_labels.append(class_id)\n",
    "            else:\n",
    "                print('图片读取失败！')\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels), classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfacd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_sift_features(images):\n",
    "    \"\"\"\n",
    "    提取彩色图像SIFT特征（分别处理RGB三个通道）\n",
    "    返回每个图像的特征描述符列表\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create()\n",
    "    all_descriptors = []\n",
    "\n",
    "    for img in images:\n",
    "        # 转换为RGB颜色空间\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        channels = cv2.split(rgb)\n",
    "        img_descriptors = []\n",
    "\n",
    "        for channel in channels:\n",
    "            _, des = sift.detectAndCompute(channel, None)\n",
    "            if des is not None:\n",
    "                img_descriptors.append(des)\n",
    "\n",
    "        # 合并三个通道的描述符\n",
    "        if len(img_descriptors) > 0:\n",
    "            # vstack 纵向合并 → 拼成 n×128 的向量\n",
    "            img_descriptors = np.vstack(img_descriptors)\n",
    "            all_descriptors.append(img_descriptors)\n",
    "        else:\n",
    "            all_descriptors.append(np.array([]))\n",
    "\n",
    "    return all_descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f990a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visual_vocabulary(descriptors_list, n_clusters=N_CLUSTERS, max_samples=MAX_SAMPLES):\n",
    "    \"\"\"\n",
    "    创建视觉词袋模型\n",
    "    \"\"\"\n",
    "    # 收集所有的描述符\n",
    "    all_descriptors = np.vstack([d for d in descriptors_list if len(d) > 0])\n",
    "\n",
    "    # 随机从样本中抽取 max_samples:\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    all_descriptors = all_descriptors[np.random.choice(len(all_descriptors), max_samples, replace=False)]\n",
    "\n",
    "    # 使用MiniBatchKMeans加速聚类\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=RANDOM_STATE)\n",
    "    kmeans.fit(all_descriptors)\n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd38e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bow_features(descriptors_list, kmeans):\n",
    "    \"\"\"\n",
    "    将描述符转换为词袋特征向量\n",
    "    \"\"\"\n",
    "    n_clusters = kmeans.n_clusters\n",
    "    features = []\n",
    "\n",
    "    for descriptors in descriptors_list:\n",
    "        # 创建归一化直方图\n",
    "        hist = np.zeros(n_clusters)\n",
    "        if len(descriptors) > 0:\n",
    "            # 关键步骤：映射到视觉单词\n",
    "            labels = kmeans.predict(descriptors)\n",
    "            # 统计每个视觉单词在当前图像中的出现次数，确保直方图的长度与视觉词典大小一致\n",
    "            hist = np.bincount(labels, minlength=n_clusters)\n",
    "\n",
    "        hist = hist.astype(np.float32)\n",
    "        hist /= hist.sum() + 1e-7  # 归一化\n",
    "\n",
    "        features.append(hist)\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aea164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    输出完整评估指标\n",
    "    \"\"\"\n",
    "    # 基础指标计算\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')  # 加权平均\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # 分类报告（包含每个类别的指标）\n",
    "    report = classification_report(y_true, y_pred, target_names=classes)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\n——— 评估结果 ———\")\n",
    "    print(f\"准确率 (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"精确率 (Precision): {precision:.4f}\")\n",
    "    print(f\"召回率 (Recall): {recall:.4f}\")\n",
    "    print(f\"F1分数 (F1-Score): {f1:.4f}\")\n",
    "\n",
    "    print(f\"\\n混淆矩阵:\\n{cm}\")\n",
    "\n",
    "    print(f\"\\n分类报告:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a368a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 4800 张, 测试集: 1200 张\n",
      "训练集特征提取完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 系统找不到指定的文件。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "d:\\300-Function\\320-Studio\\326-Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1955: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视觉词袋创建完成\n",
      "分类器训练完成\n",
      "\n",
      "——— 评估结果 ———\n",
      "准确率 (Accuracy): 0.5908\n",
      "精确率 (Precision): 0.5884\n",
      "召回率 (Recall): 0.5908\n",
      "F1分数 (F1-Score): 0.5869\n",
      "\n",
      "混淆矩阵:\n",
      "[[52  2  2  1  3  1  3  2  4  1  0  0  2  4  3]\n",
      " [ 6 47  1  5  1  2  1  3  3  0  1  2  4  3  1]\n",
      " [ 3  2 49  3  3  1  3  3  3  1  3  1  0  1  4]\n",
      " [ 0 11  2 50  0  0  0  0  1  0  0  3  4  6  3]\n",
      " [ 2  0  2  0 49  4  9  0  4  6  0  0  0  2  2]\n",
      " [ 0  0  1  0  3 67  5  0  2  0  0  0  0  0  2]\n",
      " [ 3  0  0  0 13  8 41  1  5  2  0  1  1  3  2]\n",
      " [ 4  7  2  1  1  1  7 33  7  0  0  1  8  1  7]\n",
      " [ 6  2 12  0  5  1  7  5 21  4  2  2  1  1 11]\n",
      " [ 0  0  3  1  7  0  0  2  7 56  0  0  0  2  2]\n",
      " [ 0  2  1  2  0  0  0  1  1  0 71  0  0  2  0]\n",
      " [ 1  3  2 14  0  0  0  2  4  1  2 42  3  1  5]\n",
      " [ 0 11  0  8  1  0  0 11  1  0  0  4 44  0  0]\n",
      " [ 3  4  1  0  2  0  0  2  3  0  1  0  0 62  2]\n",
      " [ 9  2 14  2  3  4  3  1  5  7  0  3  1  1 25]]\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Agriculture       0.58      0.65      0.62        80\n",
      "     Airport       0.51      0.59      0.54        80\n",
      "       Beach       0.53      0.61      0.57        80\n",
      "        City       0.57      0.62      0.60        80\n",
      "      Desert       0.54      0.61      0.57        80\n",
      "      Forest       0.75      0.84      0.79        80\n",
      "   Grassland       0.52      0.51      0.52        80\n",
      "     Highway       0.50      0.41      0.45        80\n",
      "        Lake       0.30      0.26      0.28        80\n",
      "    Mountain       0.72      0.70      0.71        80\n",
      "     Parking       0.89      0.89      0.89        80\n",
      "        Port       0.71      0.53      0.60        80\n",
      "     Railway       0.65      0.55      0.59        80\n",
      " Residential       0.70      0.78      0.73        80\n",
      "       River       0.36      0.31      0.34        80\n",
      "\n",
      "    accuracy                           0.59      1200\n",
      "   macro avg       0.59      0.59      0.59      1200\n",
      "weighted avg       0.59      0.59      0.59      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载并划分数据集\n",
    "dataset_path = \"../Aerial_Landscapes\"\n",
    "# dataset_path = \"/kaggle/input/skyview-an-aerial-landscape-dataset/Aerial_Landscapes\"\n",
    "# 每类只使用50%数据\n",
    "(train_images, train_labels), (test_images, test_labels), classes = (\n",
    "    load_and_split_dataset(dataset_path, test_size=TEST_SIZE, sample_ratio=0.5)\n",
    ")\n",
    "\n",
    "print(f\"训练集: {len(train_images)} 张, 测试集: {len(test_images)} 张\")\n",
    "\n",
    "# 2. 提取训练集特征\n",
    "train_descriptors = extract_color_sift_features(train_images)\n",
    "print(\"训练集特征提取完成\")\n",
    "\n",
    "# 3. 创建视觉词袋（仅用训练集数据！）\n",
    "kmeans = create_visual_vocabulary(train_descriptors)\n",
    "print(\"视觉词袋创建完成\")\n",
    "\n",
    "# 4. 转换训练集和测试集为BOW特征\n",
    "X_train = extract_bow_features(train_descriptors, kmeans)\n",
    "X_test = extract_bow_features(extract_color_sift_features(test_images), kmeans)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# 5. 标准化（仅用训练集统计量）\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 6. train SVM classifier\n",
    "clf = SVC(kernel=\"linear\", random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"分类器训练完成\")\n",
    "\n",
    "# 7. evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "evaluate_model(y_test, y_pred, classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
